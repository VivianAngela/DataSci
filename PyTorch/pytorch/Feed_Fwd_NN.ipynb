{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Feed-forward Neural Network with PyTorch\n",
    "\n",
    "## 1. About Feed-forward Networks\n",
    "\n",
    "### 1.1 Logistic Regression Transition to Neural Networks\n",
    "\n",
    "**Logistic Regression Review**\n",
    "(Same graphic illustrating the 4 major steps of log. regression i.e. linear regression to get logits, applied to softmax function, which generates probabilities of targets belonging to any number of classes, which then gives us a label to assign to the target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "output_dim = 10\n",
    "\n",
    "model = LogisticRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel (\n",
      "  (linear): Linear (784 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the model shows that the above model consists of a single linear layer that takes a vector of size 784 and outputs a vector of 10 (our digit classes 0-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Pitfalls**\n",
    "* Can represent linear functions well\n",
    "    * $y=2x+3$\n",
    "    * $y=x_1+x_2$\n",
    "    * $y=x_1+3x_2+4x_3$\n",
    "* Can **not** represent **non-linear** functions\n",
    "    * $y=4x_1+2x_2^2+3x_3^2$\n",
    "    * $y=x_1x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introducing a Non-Linear Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, he starts with a graphic that shows how a feed-forward neural network squeezes in a hidden layer between the input layer and the linear function layer of logistic regression. The group of layers from logistic regression that consists of the logits and the softmax layer are considered the readout layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Non-linear Function In-depth\n",
    "* Function: takes a number and performs a mathematical operation\n",
    "* Common Types of Non-linearity:\n",
    "    * ReLU (Rectified Linear Unit)\n",
    "    * Sigmoid\n",
    "    * Tanh\n",
    "    \n",
    "**Sigmoid (Logistic)**\n",
    "* $\\sigma(x) = \\frac1{1+e^{-x}}$\n",
    "* Input number $\\to$ [0,1]\n",
    "    * Large negative number $\\to$ 0\n",
    "    * Large positive number $\\to$ 1\n",
    "* Cons:\n",
    "    1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        * No signal to update weights $\\to$ **cannot learn**\n",
    "        * Solution: have to carefully initialize weights to prevent this\n",
    "    - Outputs not centered around 0\n",
    "        * If output is always positive $\\to$ gradients always positive or negative $\\to$ bad for gradient updates\n",
    "        \n",
    "**Tanh**\n",
    "* $tanh(x)=2\\sigma2x-1$\n",
    "    * A scaled sigmoid function (see above section\n",
    "* Input number $\\to$ [-1,1]\n",
    "    * Cons:\n",
    "        1. Activation saturates at 0 or 1 with **gradients $\\approx$ 0**\n",
    "        * No signal to update weights $\\to$ **cannot learn**\n",
    "        * Solution: have to carefully initialize weights to prevent this\n",
    "        \n",
    "**ReLU**\n",
    "* $f(x)=max(0,1)$\n",
    "* Pros:\n",
    "    1. Accelerates convergence $\\to$ train **faster**\n",
    "    - **Less computationally expensive operation** compared to Sigmoid/Tanh exponentials\n",
    "* Cons:\n",
    "    1. Many ReLUs \"die\" $\\to$ gradients = 0 forever\n",
    "        * Solution: Careful learning rate choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Feed-forward Neural Network with PyTorch\n",
    "\n",
    "### Model A: 1 Hidden Layer Feed-forward Neural Network (Sigmoid Activation)\n",
    "\n",
    "Back to graphic showing the single layer neural network with the readout function using softmax / logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "* Step 1: Load Dataset\n",
    "* Step 2: Make Dataset Iterable\n",
    "* Step 3: Create Model Class\n",
    "* Step 4: Instantiate Model Class\n",
    "* Step 5: Instantiate Loss Class\n",
    "* Step 6: Instantiate Optimizer Class\n",
    "* Step 7: Train Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load MNIST Dataset\n",
    "\n",
    "Images from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root=\"./data\",\n",
    "                            train=True,\n",
    "                            transform=transforms.ToTensor(), \n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Make Dataset Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "n_iters = 3000\n",
    "num_epochs = int(n_iters / (len(train_dataset) / batch_size))  #Return as type INT\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNeuralNetModel, self).__init__()\n",
    "        #Linear Function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-Linearity\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #Linear Function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Linear Function\n",
    "        out = self.fc1(x)\n",
    "        #Non-Linearity\n",
    "        out = self.sigmoid(out)\n",
    "        #Linear Func. (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Instantiate Model Class\n",
    "\n",
    "* **Input** Dimension: **784**\n",
    "    * Size of image\n",
    "    * $28\\times28 = 784$\n",
    "* **Output** Dimension: **10**\n",
    "    * 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
    "* **Hidden** Dimension: **100**\n",
    "    * Can be any number\n",
    "    * Similar term\n",
    "        * Number of neurons\n",
    "        * Number of non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedForwardNeuralNetModel(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Instantiate Loss Class\n",
    "\n",
    "* Feed Forward Neural Network: **Cross-Entropy Loss**\n",
    "    * Logistic Regression: **Cross-Entropy Loss**\n",
    "    * Linear Regression: **MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # This calculates our softmax automatically, which is why we don't have it in the model def."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Instantiate Optimizer Class\n",
    "\n",
    "* Simplified Equation\n",
    "    * $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta$\n",
    "        * $\\theta$: Parameters (our variables)\n",
    "        * $\\eta$: Learning Rate\n",
    "        * $\\nabla_\\theta$: Our Parameters' Gradients\n",
    "* Even Simpler Equation\n",
    "    * parameters = parameters - learning rate * parameters' gradients\n",
    "    * **At every iteration, we update our model's parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters In-depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x1108fed00>\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(model.parameters())\n",
    "\n",
    "print(len(list(model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 784])\n",
      "torch.Size([100])\n",
      "torch.Size([10, 100])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# FC 1 Parameters\n",
    "print(list(model.parameters())[0].size())\n",
    "\n",
    "# FC 1 Bias Parameters\n",
    "print(list(model.parameters())[1].size())\n",
    "\n",
    "# FC 2 Parameters\n",
    "print(list(model.parameters())[2].size())\n",
    "\n",
    "# FC 2 Bias Parameters\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He explains the above using another graphic. Basically, he walks through the matrix math that allows us to arrive at the solution. Matrix of 100 images (784,1) going in. 10 probabilities coming out the other end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the model\n",
    "* Process\n",
    "    1. Convert inputs/labels to variables\n",
    "    - Clear gradient buffers\n",
    "    - Get outputs given inputs\n",
    "    - Get Loss\n",
    "    - Get gradients w.r.t. parameters\n",
    "    - Update parameters using gradients\n",
    "        * parameters = parameters - learning_rate * parameters_gradients\n",
    "    - REPEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.5383222103118896. Accuracy: 85.51\n",
      "Iteration: 1000. Loss: 0.3562361001968384. Accuracy: 89.39\n",
      "Iteration: 1500. Loss: 0.2724452316761017. Accuracy: 90.46\n",
      "Iteration: 2000. Loss: 0.2643623650074005. Accuracy: 91.16\n",
      "Iteration: 2500. Loss: 0.2329949140548706. Accuracy: 91.52\n",
      "Iteration: 3000. Loss: 0.3233594000339508. Accuracy: 91.89\n"
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get outputs / logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> Cross Entropy Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Get gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0 \n",
    "            total = 0\n",
    "            #Iterate through the test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "                \n",
    "                # Forward pass only to get outputs/logits\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not to shabby using sigmoid activation. However, as we'll see later, using the sigmoid function for activation is pretty good, but not the best. Below, we explore some of the other types of activation functions ans compare their effectiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model B: 1 Hidden Layer Feed Forward Neural Network (Tanh Activation)\n",
    "\n",
    "### Steps\n",
    "\n",
    "* Step 1: Load Dataset\n",
    "* Step 2: Make Dataset Iterable\n",
    "* Step 3: **Create Model Class**\n",
    "* Step 4: Instantiate Model Class\n",
    "* Step 5: Instantiate Loss Class\n",
    "* Step 6: Instantiate Optimizer Class\n",
    "* Step 7: Train Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.3427676856517792. Accuracy: 91.35\n",
      "Iteration: 1000. Loss: 0.2686253786087036. Accuracy: 92.57\n",
      "Iteration: 1500. Loss: 0.18530148267745972. Accuracy: 93.35\n",
      "Iteration: 2000. Loss: 0.1924872249364853. Accuracy: 94.05\n",
      "Iteration: 2500. Loss: 0.2858823537826538. Accuracy: 94.74\n",
      "Iteration: 3000. Loss: 0.16669431328773499. Accuracy: 95.18\n"
     ]
    }
   ],
   "source": [
    "# Instructor had all the code from the beginning (even imports) recreated. I'm just going to change the model\n",
    "# definition part and run through to the end from there. I think I understand enough so I don't have to waste \n",
    "# time / space rewriting each section, when I can change just the relevant section\n",
    "\n",
    "'''\n",
    "Step 3: Create Model Class (tanh)\n",
    "'''\n",
    "\n",
    "class FeedForwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNeuralNetModel, self).__init__()\n",
    "        #Linear Function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-Linearity\n",
    "        self.tanh = nn.Tanh()\n",
    "        #Linear Function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Linear Function\n",
    "        out = self.fc1(x)\n",
    "        #Non-Linearity\n",
    "        out = self.tanh(out)\n",
    "        #Linear Func. (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Step 4: Instantiate Model Class\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedForwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "Step 5: Instantiate Loss Class\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate Optimizer Class\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "Step 7: Train the tanh model\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get outputs / logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> Cross Entropy Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Get gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0 \n",
    "            total = 0\n",
    "            #Iterate through the test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "                \n",
    "                # Forward pass only to get outputs/logits\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model C: 1 Hidden Layer Feed Forward Neural Network (ReLU Activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.3681732416152954. Accuracy: 91.41\n",
      "Iteration: 1000. Loss: 0.2608332931995392. Accuracy: 92.89\n",
      "Iteration: 1500. Loss: 0.14521712064743042. Accuracy: 94.03\n",
      "Iteration: 2000. Loss: 0.1494460254907608. Accuracy: 94.79\n",
      "Iteration: 2500. Loss: 0.13547107577323914. Accuracy: 95.41\n",
      "Iteration: 3000. Loss: 0.3660094141960144. Accuracy: 95.72\n"
     ]
    }
   ],
   "source": [
    "# Instructor had all the code from the beginning (even imports) recreated. I'm just going to change the model\n",
    "# definition part and run through to the end from there. I think I understand enough so I don't have to waste \n",
    "# time / space rewriting each section, when I can change just the relevant section\n",
    "\n",
    "'''\n",
    "Step 3: Create Model Class (relu)\n",
    "'''\n",
    "\n",
    "class FeedForwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNeuralNetModel, self).__init__()\n",
    "        #Linear Function\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-Linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        #Linear Function (readout)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Linear Function\n",
    "        out = self.fc1(x)\n",
    "        #Non-Linearity\n",
    "        out = self.relu(out)\n",
    "        #Linear Func. (readout)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Step 4: Instantiate Model Class\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedForwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "Step 5: Instantiate Loss Class\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate Optimizer Class\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "Step 7: Train the ReLU model\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get outputs / logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> Cross Entropy Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Get gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0 \n",
    "            total = 0\n",
    "            #Iterate through the test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "                \n",
    "                # Forward pass only to get outputs/logits\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! ReLU activation seems faster, and it also returns the best accuracy so far. I made quite a bit of progress today. Also, it seems that After defining the new model, it is necessary to re-do each of the subsequent steps in order. Once I did that, I was able to recreate the Instructor's results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D: 2 Hidden Layer Feed Forward Neural Network (ReLU Activation)\n",
    "\n",
    "We can keep stacking layers with Feed Forward NN's. Now, we're starting to get to the \"deep\" part of deep learning. Here's an example. Note the difference in step 3, creating the model class.\n",
    "\n",
    "### Steps\n",
    "\n",
    "* Step 1: Load Dataset\n",
    "* Step 2: Make Dataset Iterable\n",
    "* Step 3: **Create Model Class**\n",
    "* Step 4: Instantiate Model Class\n",
    "* Step 5: Instantiate Loss Class\n",
    "* Step 6: Instantiate Optimizer Class\n",
    "* Step 7: Train Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.29816877841949463. Accuracy: 91.45\n",
      "Iteration: 1000. Loss: 0.3057694137096405. Accuracy: 93.86\n",
      "Iteration: 1500. Loss: 0.12594181299209595. Accuracy: 94.52\n",
      "Iteration: 2000. Loss: 0.19790372252464294. Accuracy: 95.87\n",
      "Iteration: 2500. Loss: 0.09370218217372894. Accuracy: 96.53\n",
      "Iteration: 3000. Loss: 0.056080807000398636. Accuracy: 96.73\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 3: Create Model Class (relu)\n",
    "'''\n",
    "\n",
    "class FeedForwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNeuralNetModel, self).__init__()\n",
    "        #Linear Function 1: 785 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-Linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #Linear Function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #Non-Linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        #Linear Function 3: 100 --> 10 (readout)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Linear Function 1\n",
    "        out = self.fc1(x)\n",
    "        #Non-Linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        #Linear Function 2\n",
    "        out = self.fc2(out)\n",
    "        #Non-Linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        #Linear Func. (readout)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Step 4: Instantiate Model Class\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedForwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "Step 5: Instantiate Loss Class\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate Optimizer Class\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "Step 7: Train the ReLU model\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get outputs / logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> Cross Entropy Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Get gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0 \n",
    "            total = 0\n",
    "            #Iterate through the test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "                \n",
    "                # Forward pass only to get outputs/logits\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model E: 3 Hidden Layer Feed Forward Neural Network (ReLU Activation)\n",
    "\n",
    "Let's stack another layer into our model. This reminds me of the \"Sequential\" model building in Keras. Very intuitive - the bonus with PyTorch is that you can see everything in front of your face (at least so far). Clean and simple.\n",
    "\n",
    "This example may show that adding layers doesn't necessarily mean better accuracy. There's a tradeoff to weigh between accuracy and extraneous parameters to train & computation cost/time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 500. Loss: 0.27787747979164124. Accuracy: 91.3\n",
      "Iteration: 1000. Loss: 0.2696903944015503. Accuracy: 94.33\n",
      "Iteration: 1500. Loss: 0.12281842529773712. Accuracy: 95.68\n",
      "Iteration: 2000. Loss: 0.05858999863266945. Accuracy: 95.73\n",
      "Iteration: 2500. Loss: 0.11681151390075684. Accuracy: 96.47\n",
      "Iteration: 3000. Loss: 0.03441615030169487. Accuracy: 96.99\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Step 3: Create Model Class (relu)\n",
    "'''\n",
    "\n",
    "class FeedForwardNeuralNetModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(FeedForwardNeuralNetModel, self).__init__()\n",
    "        #Linear Function 1: 785 --> 100\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        #Non-Linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        #Linear Function 2: 100 --> 100\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #Non-Linearity 2\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "        #Linear Function 3: 100 --> 100\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        #Non-Linearity 3\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        #Linear Function 4: 100 --> 10 (readout)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #Linear Function 1\n",
    "        out = self.fc1(x)\n",
    "        #Non-Linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        #Linear Function 2\n",
    "        out = self.fc2(out)\n",
    "        #Non-Linearity 2\n",
    "        out = self.relu2(out)\n",
    "        \n",
    "        #Linear Function 3\n",
    "        out = self.fc3(out)\n",
    "        #Non-Linearity 3\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        #Linear Func. 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "'''\n",
    "Step 4: Instantiate Model Class\n",
    "'''\n",
    "input_dim = 28*28\n",
    "hidden_dim = 100\n",
    "output_dim = 10\n",
    "\n",
    "model = FeedForwardNeuralNetModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "'''\n",
    "Step 5: Instantiate Loss Class\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "Step 6: Instantiate Optimizer Class\n",
    "'''\n",
    "\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "Step 7: Train the ReLU model\n",
    "'''\n",
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as Variable\n",
    "        images = Variable(images.view(-1,28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        #Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Forward pass to get outputs / logits\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate Loss: softmax --> Cross Entropy Loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Get gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter % 500 == 0:\n",
    "            # Calculate Accuracy\n",
    "            correct = 0 \n",
    "            total = 0\n",
    "            #Iterate through the test dataset\n",
    "            for images, labels in test_loader:\n",
    "                # Load images to Torch Variable\n",
    "                images = Variable(images.view(-1,28*28))\n",
    "                \n",
    "                # Forward pass only to get outputs/logits\n",
    "                outputs = model(images)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "                \n",
    "            accuracy = 100 * correct / total\n",
    "            \n",
    "            # Print Loss\n",
    "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.data[0], accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that adding another layer barely registered on the overall accuracy of the model. No extra value added from the extra complications of a third layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: Deep Learning\n",
    "* 2 ways to expand a neural network\n",
    "    * more non-linear activation units (neurons) i.e. make it wider\n",
    "    * more hidden layers i.e. make it deeper\n",
    "* Cons\n",
    "    * More layers / neurons require more input data, or else benefits diminish quickly\n",
    "        * \"Curse of dimensionality\"\n",
    "    * Does not necessarily mean higher accuracy (see model E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
